{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pyagrum as gum\n",
    "\n",
    "from abapc import ABAPC\n",
    "from utils.helpers import random_stability\n",
    "\n",
    "\n",
    "dataset_size = {\n",
    "    \"asia\": \"small\",\n",
    "    \"cancer\": \"small\",\n",
    "    \"earthquake\": \"small\",\n",
    "    \"sachs\": \"small\",\n",
    "    \"survey\": \"small\",\n",
    "    # \"child\": \"medium\",  # We skip child dataset as it has irregular characters causing pyagrum parse errors\n",
    "    \"insurance\": \"medium\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    bif_path: Path, sample_size: int, seed: int\n",
    ") -> tuple[gum.BayesNet, np.ndarray]:\n",
    "    bn = gum.loadBN(str(bif_path))\n",
    "    bn.name = bif_path.stem\n",
    "\n",
    "    gum.initRandom(seed=seed)\n",
    "    df = gum.generateSample(bn, sample_size, with_labels=False, random_order=False)[0]\n",
    "    sorted_vars = sorted(df.columns)\n",
    "    data = df[sorted_vars].to_numpy().astype(float)\n",
    "\n",
    "    return bn, data\n",
    "\n",
    "\n",
    "def extract_facts(string: str) -> list[dict]:\n",
    "    pattern = re.compile(r\"ext_((in)?dep)\\((.+)\\). I=([01].\\d+), NA\\n\")\n",
    "    matches = pattern.findall(string)\n",
    "    facts = []\n",
    "    for match in matches:\n",
    "        cit_type, _, triple, score = match\n",
    "        X, Y, S = triple.split(\",\")\n",
    "        facts.append(\n",
    "            {\n",
    "                \"cit_type\": cit_type,\n",
    "                \"X\": int(X),\n",
    "                \"Y\": int(Y),\n",
    "                \"S\": set() if S == \"empty\" else {int(var) for var in S[1:].split(\"y\")},\n",
    "                \"score\": float(score),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(facts).sort_values(\n",
    "        by=\"score\", ascending=False, ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab950870",
   "metadata": {},
   "source": [
    "## Evaluation of the external facts ranking using Average Precision\n",
    "\n",
    "Let's use Asia dataset as an example to compare the ranking of the following configurations:\n",
    "- S_weight = True, cit method = \"fisherz\"\n",
    "- S_weight = False, cit method = \"fisherz\"\n",
    "- S_weight = True, cit method = \"gsq\"\n",
    "- S_weight = False, cit method = \"gsq\"\n",
    "\n",
    "Here `S_weight` represents whether to penalise the size of the conditioning set, and `cit method` refers to the conditional independence test method used in the MPC algorithm.\n",
    "\n",
    "We use `Average Precision (AP)` and `Normalized Discounted Cumulative Gain (NDCG)` as the evaluation metrics, they are common recommendation system metrics that evaluate the quality of ranked lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aaa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "\n",
    "\n",
    "repeats = {\n",
    "    \"small\": 50,\n",
    "    \"medium\": 50,\n",
    "}\n",
    "sample_size = 5000\n",
    "\n",
    "ranking_metrics = []\n",
    "for dataset, size in dataset_size.items():\n",
    "    bif_path = Path(f\"datasets/bayesian/{size}/{dataset}.bif/{dataset}.bif\")\n",
    "    random_stability(2024)\n",
    "    seeds = np.random.randint(0, 10000, size=repeats[size]).tolist()\n",
    "    for S_weight in [True, False]:\n",
    "        for cit_method in [\"fisherz\", \"gsq\"]:\n",
    "            total_average_precision = 0.0\n",
    "            total_ndcg = 0.0\n",
    "            for seed in seeds:\n",
    "                bn_true, data = load_dataset(\n",
    "                    bif_path, sample_size=sample_size, seed=seed\n",
    "                )\n",
    "                facts_I_path, _ = ABAPC(\n",
    "                    data,\n",
    "                    seed=seed,\n",
    "                    alpha=0.01,\n",
    "                    indep_test=cit_method,\n",
    "                    S_weight=S_weight,\n",
    "                    pre_grounding=True,\n",
    "                    out_mode=\"facts_only\",\n",
    "                    scenario=f\"ranking_test_{dataset}\",\n",
    "                )\n",
    "                sorted_vars = sorted(bn_true.names())\n",
    "                df = extract_facts(Path(facts_I_path).read_text())\n",
    "\n",
    "                df[\"X\"] = df[\"X\"].apply(lambda x: sorted_vars[x])\n",
    "                df[\"Y\"] = df[\"Y\"].apply(lambda y: sorted_vars[y])\n",
    "                df[\"S\"] = df[\"S\"].apply(lambda s: {sorted_vars[i] for i in s})\n",
    "                df[\"correct\"] = df.apply(\n",
    "                    lambda row: (\n",
    "                        row[\"cit_type\"] == \"indep\"\n",
    "                        if bn_true.isIndependent(row[\"X\"], row[\"Y\"], row[\"S\"])\n",
    "                        else (row[\"cit_type\"] == \"dep\")\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "                total_average_precision += average_precision_score(\n",
    "                    df[\"correct\"], df[\"score\"]\n",
    "                )\n",
    "                total_ndcg += ndcg_score([df[\"correct\"]], [df[\"score\"]])\n",
    "\n",
    "            ranking_metrics.append(\n",
    "                (\n",
    "                    dataset,\n",
    "                    S_weight,\n",
    "                    cit_method,\n",
    "                    total_average_precision / repeats[size],\n",
    "                    total_ndcg / repeats[size],\n",
    "                )\n",
    "            )\n",
    "\n",
    "ranking_df = pd.DataFrame(\n",
    "    ranking_metrics,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"S_weight\",\n",
    "        \"cit_method\",\n",
    "        \"mean_average_precision\",\n",
    "        \"mean_ndcg\",\n",
    "    ],\n",
    ")\n",
    "ranking_df.to_csv(\"results/2025/ranking_metrics.csv\", index=False)\n",
    "ranking_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fb360",
   "metadata": {},
   "source": [
    "## Computation Efficiency Comparison\n",
    "\n",
    "Let's compare the original implementation with the optimized one in terms of computation time on conflicts resolution. We use the same external facts generated on the setting `S_weight = False` and `cit method = gsq`, and `disable weak constraints`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61651348",
   "metadata": {},
   "source": [
    "### Original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied and modifed from\n",
    "# https://github.com/briziorusso/ArgCausalDisco/blob/579a1c95f576ecb53d97b0c62d4ceebb90368189/causalaba.py\n",
    "import os\n",
    "import logging\n",
    "from clingo.control import Control\n",
    "from clingo import Number, Function\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.graph_utils import powerset, extract_test_elements_from_symbol\n",
    "from priors.generate_priors import Constraints, PriorKnowledge\n",
    "\n",
    "\n",
    "def original_compile_and_ground(n_nodes:int, facts_location:str=\"\",\n",
    "                skeleton_rules_reduction:bool=False,\n",
    "                weak_constraints:bool=False,\n",
    "                indep_facts:set=set(),\n",
    "                dep_facts:set=set(),\n",
    "                opt_mode:str='optN',\n",
    "                show:list=['arrow'],\n",
    "                *args,\n",
    "                **kwargs,  # Ignore parameters introduced in the new implementation\n",
    "                )->Control:\n",
    "\n",
    "    logging.info(f\"Compiling the program\")\n",
    "    ### Create Control\n",
    "    ctl = Control(['-t %d' % os.cpu_count()])\n",
    "    ctl.configuration.solve.parallel_mode = os.cpu_count()\n",
    "    ctl.configuration.solve.models=1\n",
    "    ctl.configuration.solver.seed=\"2024\"\n",
    "    ctl.configuration.solve.opt_mode = opt_mode\n",
    "\n",
    "    ### Add set definition\n",
    "    for S in powerset(range(n_nodes)):\n",
    "        for s in S:\n",
    "            ctl.add(\"specific\", [], f\"in({s},{'s'+'y'.join([str(i) for i in S])}).\")\n",
    "            # logging.debug(f\"in({s},{'s'+'y'.join([str(i) for i in S])}).\")\n",
    "\n",
    "    ### Load main program and facts\n",
    "    ctl.load(\"./encodings/causalaba.lp\")\n",
    "    if facts_location != \"\":\n",
    "        ctl.load(facts_location)\n",
    "    if weak_constraints:\n",
    "        ctl.load(facts_location.replace(\".lp\",\"_wc.lp\"))\n",
    "\n",
    "    ctl.add(\"specific\", [], \"indep(X,Y,S) :- ext_indep(X,Y,S), var(X), var(Y), set(S), X!=Y.\")\n",
    "    ctl.add(\"specific\", [], \"dep(X,Y,S) :- ext_dep(X,Y,S), var(X), var(Y), set(S), X!=Y.\")\n",
    "    ### add nonblocker rules\n",
    "    logging.info(\"   Adding Specific Rules...\")\n",
    "\n",
    "    ### Active paths rules\n",
    "    n_p = 0\n",
    "    G = nx.complete_graph(n_nodes)\n",
    "    for (X,Y) in combinations(range(n_nodes),2):\n",
    "        paths = nx.all_simple_paths(G, source=X, target=Y)\n",
    "        ### remove paths that contain an indep fact\n",
    "        paths_mat = np.array([np.array(list(xi)+[None]*(n_nodes-len(xi))) for xi in paths])\n",
    "        if skeleton_rules_reduction:\n",
    "            paths_mat_red = paths_mat[[not any([(paths_mat[i,j],paths_mat[i,j+1]) in indep_facts or\n",
    "                                                (paths_mat[i,j+1],paths_mat[i,j]) in indep_facts\n",
    "                                                for j in range(n_nodes-1) if paths_mat[i,j] is not None]) \\\n",
    "                                                    for i in range(len(paths_mat))]] ##TODO: think about interaction with weak constraints\n",
    "            remaining_paths = [list(filter(lambda x: x is not None, paths_mat_red[i])) for i in range(len(paths_mat_red))]\n",
    "            logging.debug(f\"Paths from {X} to {Y}: {len(paths_mat)}, removing indep: {len(remaining_paths)}\")#\n",
    "            excluded_paths = [list(filter(lambda x: x is not None, paths_mat[i])) for i in range(len(paths_mat)) if any([(paths_mat[i,j],paths_mat[i,j+1]) in indep_facts for j in range(n_nodes-1) if paths_mat[i,j] is not None])]\n",
    "            logging.debug(f\"Excluded paths from {X} to {Y}: {len(excluded_paths)}\")\n",
    "        else:\n",
    "            remaining_paths = [list(filter(lambda x: x is not None, paths_mat[i])) for i in range(len(paths_mat))]\n",
    "\n",
    "        indep_rule_body = []\n",
    "        for path in remaining_paths:\n",
    "            n_p += 1\n",
    "            ### build indep rule body\n",
    "            indep_rule_body.append(f\" not ap({X},{Y},p{n_p},S)\")\n",
    "\n",
    "            ### add path rule\n",
    "            path_edges = [f\"edge({path[idx]},{path[idx+1]})\" for idx in range(len(path)-1)]\n",
    "            ctl.add(\"specific\", [], f\"p{n_p} :- {','.join(path_edges)}.\")\n",
    "            logging.debug(f\"   p{n_p} :- {','.join(path_edges)}.\")\n",
    "\n",
    "            ### add active path rule\n",
    "            nbs = [f\"nb({path[idx]},{path[idx-1]},{path[idx+1]},S)\" for idx in range(1,len(path)-1)]\n",
    "            nbs_str = ','.join(nbs)+\",\" if len(nbs) > 0 else \"\"\n",
    "            ctl.add(\"specific\", [], f\"ap({X},{Y},p{n_p},S) :- p{n_p}, {nbs_str} not in({X},S), not in({Y},S), set(S).\")\n",
    "            logging.debug(f\"   ap({X},{Y},p{n_p},S) :- p{n_p}, {nbs_str} not in({X},S), not in({Y},S), set(S).\")\n",
    "\n",
    "        ### add indep rule\n",
    "        indep_rule_body = \"\" if not indep_rule_body else f\"{','.join(indep_rule_body)}, \"\n",
    "        if skeleton_rules_reduction:\n",
    "            if (X,Y) in dep_facts:\n",
    "                indep_rule = f\"indep({X},{Y},S) :- {indep_rule_body}not in({X},S), not in({Y},S), set(S).\"\n",
    "                ctl.add(\"specific\", [], indep_rule)\n",
    "                logging.debug(   indep_rule)\n",
    "        else:\n",
    "            indep_rule = f\"indep({X},{Y},S) :- {indep_rule_body} not in({X},S), not in({Y},S), set(S).\"\n",
    "            ctl.add(\"specific\", [], indep_rule)\n",
    "            logging.debug(   indep_rule)\n",
    "\n",
    "    ### add dep rule\n",
    "    ctl.add(\"specific\", [], f\"dep(X,Y,S):- ap(X,Y,_,S), var(X), var(Y), X!=Y, not in(X,S), not in(Y,S), set(S).\")\n",
    "    logging.debug(   f\"dep(X,Y,S) :- ap(X,Y,_,S), var(X), var(Y), X!=Y, not in(X,S), not in(Y,S), set(S).\")\n",
    "\n",
    "    ### add show statements\n",
    "    if 'arrow' in show:\n",
    "        ctl.add(\"base\", [], \"#show arrow/2.\")\n",
    "    if 'indep' in show:\n",
    "        ctl.add(\"base\", [], \"#show indep/3.\")\n",
    "    if 'dep' in show:\n",
    "        ctl.add(\"base\", [], \"#show dep/3.\")\n",
    "    if 'collider' in show:\n",
    "        ctl.add(\"base\", [], \"#show collider/3.\")\n",
    "    if 'collider_desc' in show:\n",
    "        ctl.add(\"base\", [], \"#show collider_desc/4.\")\n",
    "    if 'nb' in show:\n",
    "        ctl.add(\"base\", [], \"#show nb/4.\")\n",
    "    if 'ap' in show:\n",
    "        ctl.add(\"base\", [], \"#show ap/4.\")\n",
    "    if 'dpath' in show:\n",
    "        ctl.add(\"base\", [], \"#show dpath/2.\")\n",
    "\n",
    "    ### Ground\n",
    "    logging.info(\"   Grounding...\")\n",
    "    start_ground = datetime.now()\n",
    "    ctl.ground([(\"base\", []), (\"facts\", []), (\"specific\", []), (\"main\", [Number(n_nodes-1)])])\n",
    "    logging.info(f\"   Grounding time: {str(datetime.now()-start_ground)}\")\n",
    "\n",
    "    return ctl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d283c2",
   "metadata": {},
   "source": [
    "### Define Causal ABA Interface to call both implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2df4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CausalABA(\n",
    "    n_nodes: int,\n",
    "    compile_and_ground: Callable,\n",
    "    facts_location: str = \"\",\n",
    "    print_models: bool = True,\n",
    "    skeleton_rules_reduction: bool = False,\n",
    "    weak_constraints: bool = False,\n",
    "    opt_mode: str = \"optN\",\n",
    "    show: list = [\"arrow\"],\n",
    "    pre_grounding: bool = False,\n",
    "    disable_reground: bool = False,\n",
    "    prior_knowledge: PriorKnowledge | None = None,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    CausalABA, a function that takes in the number of nodes in a graph and a string of facts and returns a list of compatible causal graphs.\n",
    "\n",
    "    \"\"\"\n",
    "    # (X, Y) -> their condition sets S\n",
    "    indep_facts: dict[tuple, set[tuple]] = {}\n",
    "    dep_facts: dict[tuple, set[tuple]] = {}\n",
    "    facts = []\n",
    "    ext_flag = False\n",
    "    if facts_location:\n",
    "        facts_loc = (\n",
    "            facts_location.replace(\".lp\", \"_I.lp\")\n",
    "            if weak_constraints\n",
    "            else facts_location\n",
    "        )\n",
    "        logging.debug(f\"   Loading facts from {facts_location}\")\n",
    "        with open(facts_loc, \"r\") as file:\n",
    "            for line in file:\n",
    "                if \"dep\" not in line or line.startswith(\"%\"):\n",
    "                    continue\n",
    "                line_clean = line.replace(\"#external \", \"\").replace(\"\\n\", \"\")\n",
    "                if \"ext_\" in line_clean:\n",
    "                    ext_flag = True\n",
    "                if weak_constraints:\n",
    "                    statement, Is = line_clean.split(\" I=\")\n",
    "                    I, truth = Is.split(\",\")\n",
    "                    X, S, Y, dep_type = extract_test_elements_from_symbol(statement)\n",
    "                    facts.append((X, S, Y, dep_type, statement, float(I), truth))\n",
    "                else:\n",
    "                    X, S, Y, dep_type = extract_test_elements_from_symbol(line_clean)\n",
    "                    facts.append((X, S, Y, dep_type, line_clean, np.nan, \"unknown\"))\n",
    "\n",
    "                assert (X not in S) and (Y not in S), f\"X or Y in S: {line_clean}\"\n",
    "                condition_set = tuple(S)\n",
    "\n",
    "                facts_group = indep_facts if \"indep\" in line_clean else dep_facts\n",
    "                if (X, Y) not in facts_group:\n",
    "                    facts_group[(X, Y)] = set()\n",
    "                assert condition_set not in facts_group[(X, Y)], (\n",
    "                    f\"Redundant external fact: {line_clean}\"\n",
    "                )\n",
    "                facts_group[(X, Y)].add(condition_set)\n",
    "\n",
    "    ctl = compile_and_ground(\n",
    "        n_nodes,\n",
    "        facts_location,\n",
    "        skeleton_rules_reduction,\n",
    "        weak_constraints,\n",
    "        indep_facts,\n",
    "        dep_facts,\n",
    "        opt_mode,\n",
    "        show,\n",
    "        pre_grounding,\n",
    "        ext_flag,\n",
    "        prior_knowledge,\n",
    "    )\n",
    "\n",
    "    facts = sorted(facts, key=lambda x: x[5], reverse=True)\n",
    "    for fact in facts:\n",
    "        ctl.assign_external(\n",
    "            Function(\n",
    "                fact[3],\n",
    "                [\n",
    "                    Number(fact[0]),\n",
    "                    Number(fact[2]),\n",
    "                    Function(fact[4].replace(\").\", \"\").split(\",\")[-1]),\n",
    "                ],\n",
    "            ),\n",
    "            True,\n",
    "        )\n",
    "        logging.debug(f\"   True fact: {fact[4]} I={fact[5]}, truth={fact[6]}\")\n",
    "    models = []\n",
    "    logging.info(\"   Solving...\")\n",
    "    with ctl.solve(yield_=True) as handle:\n",
    "        for model in handle:\n",
    "            models.append(model.symbols(shown=True))\n",
    "            if print_models:\n",
    "                logging.info(f\"Answer {len(models)}: {model}\")\n",
    "    n_models = int(ctl.statistics[\"summary\"][\"models\"][\"enumerated\"])\n",
    "    logging.info(f\"Number of models: {n_models}\")\n",
    "    times = {\n",
    "        key: ctl.statistics[\"summary\"][\"times\"][key]\n",
    "        for key in [\"total\", \"cpu\", \"solve\"]\n",
    "    }\n",
    "    logging.info(f\"Times: {times}\")\n",
    "    remove_n = 0\n",
    "    logging.info(f\"Number of facts removed: {remove_n}\")\n",
    "\n",
    "    ## start removing facts if no models are found\n",
    "    while n_models == 0 and remove_n < len(facts):\n",
    "        remove_n += 1\n",
    "        logging.info(f\"Number of facts removed: {remove_n}\")\n",
    "\n",
    "        reground = False\n",
    "        fact_to_remove = facts[-remove_n]\n",
    "        X, S, Y, dep_type, fact_str = fact_to_remove[:5]\n",
    "        logging.debug(f\"Removing fact {fact_str}\")\n",
    "\n",
    "        facts_group = indep_facts if dep_type == \"ext_indep\" else dep_facts\n",
    "        facts_group[(X, Y)].remove(tuple(S))\n",
    "        if not facts_group[(X, Y)]:\n",
    "            del facts_group[(X, Y)]\n",
    "            reground = (\n",
    "                disable_reground is False\n",
    "                and skeleton_rules_reduction\n",
    "                and (ext_flag is False or dep_type == \"ext_indep\")\n",
    "            )\n",
    "        else:\n",
    "            logging.debug(\n",
    "                f\"   Not removing fact {fact_str} because there are multiple facts with the same X and Y\"\n",
    "            )\n",
    "        ctl.assign_external(\n",
    "            Function(\n",
    "                dep_type,\n",
    "                [\n",
    "                    Number(X),\n",
    "                    Number(Y),\n",
    "                    Function(fact_str.replace(\").\", \"\").split(\",\")[-1]),\n",
    "                ],\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if reground:\n",
    "            ### Save external statements\n",
    "            logging.info(\"Recompiling and regrounding...\")\n",
    "            ctl = compile_and_ground(\n",
    "                n_nodes,\n",
    "                facts_location,\n",
    "                skeleton_rules_reduction,\n",
    "                weak_constraints,\n",
    "                indep_facts,\n",
    "                dep_facts,\n",
    "                opt_mode,\n",
    "                show,\n",
    "                pre_grounding=pre_grounding,\n",
    "                ext_flag=ext_flag,\n",
    "                prior_knowledge=prior_knowledge,\n",
    "            )\n",
    "            for fact in facts[:-remove_n]:\n",
    "                ctl.assign_external(\n",
    "                    Function(\n",
    "                        fact[3],\n",
    "                        [\n",
    "                            Number(fact[0]),\n",
    "                            Number(fact[2]),\n",
    "                            Function(fact[4].replace(\").\", \"\").split(\",\")[-1]),\n",
    "                        ],\n",
    "                    ),\n",
    "                    True,\n",
    "                )\n",
    "                logging.debug(f\"   True fact: {fact[4]} I={fact[5]}, truth={fact[6]}\")\n",
    "            for fact in facts[-remove_n:]:\n",
    "                ctl.assign_external(\n",
    "                    Function(\n",
    "                        fact[3],\n",
    "                        [\n",
    "                            Number(fact[0]),\n",
    "                            Number(fact[2]),\n",
    "                            Function(fact[4].replace(\").\", \"\").split(\",\")[-1]),\n",
    "                        ],\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "                logging.debug(f\"   False fact: {fact[4]} I={fact[5]}, truth={fact[6]}\")\n",
    "        models = []\n",
    "        logging.info(\"   Solving...\")\n",
    "        with ctl.solve(yield_=True) as handle:\n",
    "            for model in handle:\n",
    "                models.append(model.symbols(shown=True))\n",
    "                if print_models:\n",
    "                    logging.info(f\"Answer {len(models)}: {model}\")\n",
    "        n_models = int(ctl.statistics[\"summary\"][\"models\"][\"enumerated\"])\n",
    "        logging.info(f\"Number of models: {n_models}\")\n",
    "        times = {\n",
    "            key: ctl.statistics[\"summary\"][\"times\"][key]\n",
    "            for key in [\"total\", \"cpu\", \"solve\"]\n",
    "        }\n",
    "        logging.info(f\"Times: {times}\")\n",
    "\n",
    "    return {\n",
    "        \"remove_n\": remove_n,\n",
    "        \"statistics\": ctl.statistics,\n",
    "        \"models\": models,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165f6bb",
   "metadata": {},
   "source": [
    "### Define the evaluation function to compare performance of both implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd46ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from typing import Any\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import causalaba\n",
    "from causalaba import compile_and_ground as new_compile_and_ground\n",
    "\n",
    "\n",
    "causalaba.tqdm = tqdm  # Avoid using progress bar widgets\n",
    "\n",
    "def original_implementation(bn: gum.BayesNet, facts_path: str, return_ref: list) -> Callable:\n",
    "    def helper():\n",
    "        res = CausalABA(\n",
    "                n_nodes=bn.size(),\n",
    "                compile_and_ground=original_compile_and_ground,\n",
    "                facts_location=facts_path,\n",
    "                print_models=False,\n",
    "                skeleton_rules_reduction=True,\n",
    "                weak_constraints=False,  # We only care about hard constraints\n",
    "                pre_grounding=False,\n",
    "                prior_knowledge=None,\n",
    "            )\n",
    "        return_ref.append(\n",
    "            {\n",
    "                \"remove_n\": res[\"remove_n\"],\n",
    "                \"atoms\": res[\"statistics\"][\"problem\"][\"lp\"][\"atoms\"],\n",
    "                \"rules\": res[\"statistics\"][\"problem\"][\"lp\"][\"rules\"],\n",
    "                \"conflicts\": res[\"statistics\"][\"solving\"][\"solvers\"][\"conflicts\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return helper\n",
    "\n",
    "def new_implementation(bn: gum.BayesNet, facts_path: str, return_ref: list) -> Callable:\n",
    "    def helper():\n",
    "        res = CausalABA(\n",
    "                n_nodes=bn.size(),\n",
    "                compile_and_ground=new_compile_and_ground,\n",
    "                facts_location=facts_path,\n",
    "                print_models=False,\n",
    "                skeleton_rules_reduction=True,\n",
    "                weak_constraints=False,  # We only care about hard constraints\n",
    "                pre_grounding=False,\n",
    "                prior_knowledge=None,\n",
    "            )\n",
    "        return_ref.append(\n",
    "            {\n",
    "                \"remove_n\": res[\"remove_n\"],\n",
    "                \"atoms\": res[\"statistics\"][\"problem\"][\"lp\"][\"atoms\"],\n",
    "                \"rules\": res[\"statistics\"][\"problem\"][\"lp\"][\"rules\"],\n",
    "                \"conflicts\": res[\"statistics\"][\"solving\"][\"solvers\"][\"conflicts\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return helper\n",
    "\n",
    "def compare_performance(\n",
    "    bif_path: str | Path,\n",
    "    sample_size: int,\n",
    "    impl1: Callable[[gum.BayesNet, str, list], Callable],\n",
    "    impl2: Callable[[gum.BayesNet, str, list], Callable],\n",
    "    repeats: int = 1,\n",
    ") -> tuple[str, int, float, float, list, list, np.ndarray, Any]:\n",
    "    random_stability(2024)\n",
    "    seeds = np.random.randint(0, 10000, size=repeats).tolist()\n",
    "\n",
    "    if isinstance(bif_path, str):\n",
    "        bif_path = Path(bif_path)\n",
    "\n",
    "    impl1_time, impl2_time = 0, 0\n",
    "    impl1_return = []\n",
    "    impl2_return = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        bn_true, data = load_dataset(bif_path, sample_size=sample_size, seed=seed)\n",
    "        facts_I_path, sepsets = ABAPC(\n",
    "            data,\n",
    "            seed=seed,\n",
    "            alpha=0.01,\n",
    "            indep_test=\"gsq\",\n",
    "            S_weight=False,\n",
    "            out_mode=\"facts_only\",\n",
    "            scenario=\"performance_comparison\",\n",
    "        )\n",
    "        facts_path = facts_I_path.replace(\"_I.lp\", \".lp\")\n",
    "\n",
    "        impl1_time += timeit.timeit(impl1(bn_true, facts_path, impl1_return), number=1)\n",
    "        impl2_time += timeit.timeit(impl2(bn_true, facts_path, impl2_return), number=1)\n",
    "\n",
    "    return bif_path.stem, bn_true.size(), impl1_time / repeats, impl2_time / repeats, impl1_return, impl2_return, seeds, sepsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6003a89",
   "metadata": {},
   "source": [
    "### Evaluate on small size bayesian networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "avg_time_res = []\n",
    "repeats = 50\n",
    "datasets = {\"asia\", \"cancer\", \"survey\", \"earthquake\"}\n",
    "for dataset, size in dataset_size.items():\n",
    "    if dataset not in datasets:\n",
    "        continue  # Due to scalability limitations of original implementation, we only test small datasets\n",
    "    (\n",
    "        bif_name,\n",
    "        graph_order,\n",
    "        impl1_avg_time,\n",
    "        impl2_avg_time,\n",
    "        impl1_return,\n",
    "        impl2_return,\n",
    "        *_,\n",
    "    ) = compare_performance(\n",
    "        bif_path=f\"datasets/bayesian/{size}/{dataset}.bif/{dataset}.bif\",\n",
    "        sample_size=5000,\n",
    "        impl1=new_implementation,\n",
    "        # Note that original_implementation will modify the files in facts_I.lp,\n",
    "        # so we call it after the new implementation\n",
    "        impl2=original_implementation,\n",
    "        repeats=repeats,\n",
    "    )\n",
    "    impl1_avg_stats = pd.DataFrame(impl1_return).add_prefix(\"new_\").mean().to_dict()\n",
    "    impl2_avg_stats = pd.DataFrame(impl2_return).add_prefix(\"org_\").mean().to_dict()\n",
    "    avg_time_res.append({\n",
    "        \"dataset\": dataset,\n",
    "        \"num_nodes\": graph_order,\n",
    "        \"new_time\": impl1_avg_time,\n",
    "        \"org_time\": impl2_avg_time,\n",
    "        \"repeats\": repeats,\n",
    "        **impl1_avg_stats,\n",
    "        **impl2_avg_stats,\n",
    "    })\n",
    "\n",
    "avg_time_res_df = pd.DataFrame(\n",
    "    avg_time_res,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"num_nodes\",\n",
    "        \"org_time\",\n",
    "        \"new_time\",\n",
    "        \"org_remove_n\",\n",
    "        \"new_remove_n\",\n",
    "        \"org_atoms\",\n",
    "        \"new_atoms\",\n",
    "        \"org_rules\",\n",
    "        \"new_rules\",\n",
    "        \"org_conflicts\",\n",
    "        \"new_conflicts\",\n",
    "        \"repeats\",\n",
    "    ],\n",
    ")\n",
    "assert avg_time_res_df[\"org_remove_n\"].equals(avg_time_res_df[\"new_remove_n\"])\n",
    "avg_time_res_df.to_csv(\"results/2025/small_bnlearn_scalability.csv\", index=False)\n",
    "avg_time_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd8885",
   "metadata": {},
   "source": [
    "### Evaluate on random bayesian networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa82ab",
   "metadata": {},
   "source": [
    "Let's compare the performance of the original and new implementations on random bayesian networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "import tempfile\n",
    "\n",
    "# Define the graph orders and their corresponding number of repeats\n",
    "graph_orders = [5, 6, 7, 8]\n",
    "repeats = 50\n",
    "avg_time_res = []\n",
    "for order in graph_orders:\n",
    "    gum.initRandom(2025)\n",
    "    random_bn = gum.randomBN(n=order, ratio_arc=1)\n",
    "    temp_bif_path = str(Path(tempfile.gettempdir()) / f\"random_bn_{order}.bif\")\n",
    "    random_bn.saveBIF(temp_bif_path)\n",
    "\n",
    "    (\n",
    "        bif_name,\n",
    "        graph_order,\n",
    "        impl1_avg_time,\n",
    "        impl2_avg_time,\n",
    "        impl1_return,\n",
    "        impl2_return,\n",
    "        *_,\n",
    "    ) = compare_performance(\n",
    "        bif_path=temp_bif_path,\n",
    "        sample_size=5000,\n",
    "        impl1=new_implementation,\n",
    "        impl2=original_implementation,\n",
    "        repeats=repeats,\n",
    "    )\n",
    "    impl1_avg_stats = pd.DataFrame(impl1_return).add_prefix(\"new_\").mean().to_dict()\n",
    "    impl2_avg_stats = pd.DataFrame(impl2_return).add_prefix(\"org_\").mean().to_dict()\n",
    "    avg_time_res.append({\n",
    "        \"dataset\": bif_name,\n",
    "        \"num_nodes\": graph_order,\n",
    "        \"new_time\": impl1_avg_time,\n",
    "        \"org_time\": impl2_avg_time,\n",
    "        \"repeats\": repeats,\n",
    "        **impl1_avg_stats,\n",
    "        **impl2_avg_stats,\n",
    "    })\n",
    "\n",
    "avg_time_res_df = pd.DataFrame(\n",
    "    avg_time_res,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"num_nodes\",\n",
    "        \"org_time\",\n",
    "        \"new_time\",\n",
    "        \"org_remove_n\",\n",
    "        \"new_remove_n\",\n",
    "        \"org_atoms\",\n",
    "        \"new_atoms\",\n",
    "        \"org_rules\",\n",
    "        \"new_rules\",\n",
    "        \"org_conflicts\",\n",
    "        \"new_conflicts\",\n",
    "        \"repeats\",\n",
    "    ],\n",
    ")\n",
    "assert avg_time_res_df[\"org_remove_n\"].equals(avg_time_res_df[\"new_remove_n\"])\n",
    "avg_time_res_df.to_csv(\"results/2025/rand_graph_scalability.csv\", index=False)\n",
    "avg_time_res_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
